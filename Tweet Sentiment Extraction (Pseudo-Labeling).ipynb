{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import softmax\n",
    "\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    DATA_DIR = '../tweet_sentiment_extraction'\n",
    "    ROBERTA_PATH = '../tweet_sentiment_extraction/roberta-base'\n",
    "    OUTPUT_DIR = '../tweet_sentiment_extraction'\n",
    "    TRAIN_FILE = 'train.csv'\n",
    "    TEST_FILE = 'test.csv'\n",
    "    FULL_DATA_FILE = 'tweet_dataset.csv'\n",
    "    PREDICT_FILE = 'predict.csv'\n",
    "    SAVE_MODEL_DIR = 'trained_model'\n",
    "    PRETRAINED_MODELS = 'warm_up_steps_100'\n",
    "    MAX_LEN = 128\n",
    "    MAX_LEN_CHAR = 140\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    VALID_BATCH_SIZE = 32\n",
    "    LOGGING_STEPS = 100\n",
    "    SEED = 1111\n",
    "    DEVICE = torch.device('cuda:1')\n",
    "    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
    "        vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n",
    "        merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n",
    "        lowercase=True,\n",
    "        add_prefix_space=True\n",
    "    )\n",
    "    SENTIMENT_MAP = {'worry':'negative', \n",
    "                     'sadness':'negative', \n",
    "                     'hate':'negative', \n",
    "                     'boredom':'negative', \n",
    "                     'anger':'negative',\n",
    "                     'relief':'positive', \n",
    "                     'happiness':'positive', \n",
    "                     'love':'positive', \n",
    "                     'surprise':'positive', \n",
    "                     'fun':'positive', \n",
    "                     'enthusiasm':'positive',\n",
    "                     'empty':'neutral',\n",
    "                     'neutral':'neutral',\n",
    "                     'positive':'positive',\n",
    "                     'negative':'negative'\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything(config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    tweet = \" \" + \" \".join(str(tweet).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    \n",
    "    tok_tweet = tokenizer.encode(tweet)\n",
    "    input_ids_orig = tok_tweet.ids\n",
    "    tweet_offsets = tok_tweet.offsets\n",
    "    \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "    \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "\n",
    "    sentiment_id = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7974\n",
    "    }\n",
    "    \n",
    "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
    "    targets_start += 4\n",
    "    targets_end += 4\n",
    "\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'targets_start': targets_start,\n",
    "        'targets_end': targets_end,\n",
    "        'orig_tweet': tweet,\n",
    "        'orig_selected': selected_text,\n",
    "        'sentiment': sentiment,\n",
    "        'offsets': tweet_offsets\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(config.DATA_DIR, config.FULL_DATA_FILE))\n",
    "train_df = train_df[train_df['selected_text'].isna()]\n",
    "train_df['new_sentiment'] = train_df['sentiment'].map(config.SENTIMENT_MAP)\n",
    "train_df = train_df.drop(['sentiment', 'aux_id', 'author', 'old_text'], axis=1)\n",
    "train_df = train_df.rename(columns={\"new_sentiment\": \"sentiment\"})\n",
    "train_df.loc[:, 'selected_text'] = train_df.text.values\n",
    "\n",
    "train_df = train_df.dropna(how='any', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset :\n",
    "    def __init__(self, tweet, sentiment, selected_text):\n",
    "        self.tweet = tweet\n",
    "        self.sentiment = sentiment\n",
    "        self.selected_text = selected_text\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_len = config.MAX_LEN\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        data = process_data(\n",
    "            self.tweet[item],\n",
    "            self.selected_text[item],\n",
    "            self.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(data['ids'], dtype=torch.long),\n",
    "            'mask': torch.tensor(data['mask'], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            'orig_tweet': data[\"orig_tweet\"],\n",
    "            'orig_selected': data[\"orig_selected\"],\n",
    "            'sentiment': data[\"sentiment\"],\n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(transformers.BertPreTrainedModel):\n",
    "    def __init__(self, conf):\n",
    "        super(TweetModel, self).__init__(conf)\n",
    "        self.roberta = transformers.RobertaModel.from_pretrained(config.ROBERTA_PATH, config=conf)\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l0 = nn.Linear(768 * 2, 200)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "        self.l1 = nn.Linear(200, 2)\n",
    "        torch.nn.init.normal_(self.l1.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, _, out = self.roberta(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out = self.drop_out(out)\n",
    "        logits = self.l0(out)\n",
    "        logits = self.l1(logits)\n",
    "\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(original_tweet, target_string, sentiment_val, idx_start, idx_end, offsets):\n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "        \n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "            \n",
    "#     if sentiment_val == 'neutral' or len(original_tweet.split()) < 2:\n",
    "    if len(original_tweet.split()) < 2:\n",
    "        filtered_output = original_tweet\n",
    "        \n",
    "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
    "        \n",
    "    return jac, filtered_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "### Get meta data model warm_up_steps_100\n",
      "###########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:75: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7e62eff4e4473495eb54ece36adba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=381.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('###########################')\n",
    "print('### Get meta data model {}'.format(config.PRETRAINED_MODELS))\n",
    "print('###########################')\n",
    "\n",
    "meta_train_df = pd.DataFrame()\n",
    "tweet_list = []\n",
    "sentiment_list = []\n",
    "final_output = []\n",
    "max_probability_start_list = []\n",
    "max_probability_end_list = []\n",
    "\n",
    "dataset = TweetDataset(\n",
    "    tweet = train_df.text.values,\n",
    "    sentiment = train_df.sentiment.values,\n",
    "    selected_text = train_df.selected_text.values\n",
    ")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = config.VALID_BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    num_workers = 8\n",
    ")\n",
    "\n",
    "model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)\n",
    "model_config.output_hidden_states = True\n",
    "\n",
    "model_path1 = os.path.join(config.DATA_DIR, \n",
    "                          config.SAVE_MODEL_DIR, \n",
    "                          config.PRETRAINED_MODELS, \n",
    "                          f'model_fold_0.bin')\n",
    "model_path2 = os.path.join(config.DATA_DIR, \n",
    "                          config.SAVE_MODEL_DIR, \n",
    "                          config.PRETRAINED_MODELS, \n",
    "                          f'model_fold_1.bin')\n",
    "model_path3 = os.path.join(config.DATA_DIR, \n",
    "                          config.SAVE_MODEL_DIR, \n",
    "                          config.PRETRAINED_MODELS, \n",
    "                          f'model_fold_2.bin')\n",
    "model_path4 = os.path.join(config.DATA_DIR, \n",
    "                          config.SAVE_MODEL_DIR, \n",
    "                          config.PRETRAINED_MODELS, \n",
    "                          f'model_fold_3.bin')\n",
    "model_path5 = os.path.join(config.DATA_DIR, \n",
    "                          config.SAVE_MODEL_DIR, \n",
    "                          config.PRETRAINED_MODELS, \n",
    "                          f'model_fold_4.bin')\n",
    "\n",
    "model1 = TweetModel(conf=model_config)\n",
    "model1.to(config.DEVICE)\n",
    "model1.load_state_dict(torch.load(model_path1))\n",
    "model1.eval()\n",
    "\n",
    "model2 = TweetModel(conf=model_config)\n",
    "model2.to(config.DEVICE)\n",
    "model2.load_state_dict(torch.load(model_path2))\n",
    "model2.eval()\n",
    "\n",
    "model3 = TweetModel(conf=model_config)\n",
    "model3.to(config.DEVICE)\n",
    "model3.load_state_dict(torch.load(model_path3))\n",
    "model3.eval()\n",
    "\n",
    "model4 = TweetModel(conf=model_config)\n",
    "model4.to(config.DEVICE)\n",
    "model4.load_state_dict(torch.load(model_path4))\n",
    "model4.eval()\n",
    "\n",
    "model5 = TweetModel(conf=model_config)\n",
    "model5.to(config.DEVICE)\n",
    "model5.load_state_dict(torch.load(model_path5))\n",
    "model5.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    tk0 = tqdm_notebook(data_loader, total=len(data_loader))\n",
    "    for bi, d in enumerate(tk0):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"].numpy() # (32, 128, 2)\n",
    "\n",
    "        ids = ids.to(config.DEVICE, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(config.DEVICE, dtype=torch.long)\n",
    "        mask = mask.to(config.DEVICE, dtype=torch.long)\n",
    "        targets_start = targets_start.to(config.DEVICE, dtype=torch.long)\n",
    "        targets_end = targets_end.to(config.DEVICE, dtype=torch.long)\n",
    "\n",
    "#                 outputs_start [32, 128]\n",
    "        outputs_start1, outputs_end1 = model1(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        outputs_start2, outputs_end2 = model2(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        outputs_start3, outputs_end3 = model3(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        outputs_start4, outputs_end4 = model4(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        outputs_start5, outputs_end5 = model5(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        outputs_start = (\n",
    "            outputs_start1\n",
    "            + outputs_start2\n",
    "            + outputs_start3\n",
    "            + outputs_start4\n",
    "            + outputs_start5\n",
    "        ) / 5\n",
    "\n",
    "        outputs_end = (\n",
    "            outputs_end1\n",
    "            + outputs_end2\n",
    "            + outputs_end3\n",
    "            + outputs_end4\n",
    "            + outputs_end5\n",
    "        ) / 5\n",
    "\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "\n",
    "            item_start = outputs_start[px, :]\n",
    "            item_end = outputs_end[px, :]\n",
    "\n",
    "            max_probability_start = np.max(item_start)\n",
    "            max_probability_end = np.max(item_end)\n",
    "\n",
    "#             if max_probability_start >= 0.8 and max_probability_end >= 0.8:\n",
    "            _, output_sentence = calculate_jaccard_score(\n",
    "                original_tweet=tweet,\n",
    "                target_string=selected_tweet,\n",
    "                sentiment_val=tweet_sentiment,\n",
    "                idx_start=np.argmax(item_start),\n",
    "                idx_end=np.argmax(item_end),\n",
    "                offsets=offsets[px]\n",
    "            )\n",
    "            \n",
    "            max_probability_start_list.append(max_probability_start)\n",
    "            max_probability_end_list.append(max_probability_end)\n",
    "            tweet_list.append(tweet)\n",
    "            sentiment_list.append(tweet_sentiment)\n",
    "            final_output.append(output_sentence)\n",
    "\n",
    "meta_train_df['text'] = tweet_list\n",
    "meta_train_df['sentiment'] = sentiment_list\n",
    "meta_train_df['selected_text'] = final_output\n",
    "meta_train_df['start_max'] = max_probability_start_list\n",
    "meta_train_df['end_max'] = max_probability_end_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>start_max</th>\n",
       "      <th>end_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No Topic Maps talks at the Balisage Markup Conference 2009 Program online at http://tr.im/mL6Z (via ) #topicmaps</td>\n",
       "      <td>neutral</td>\n",
       "      <td>No Topic Maps talks at the Balisage Markup Conference 2009 Program online at http://tr.im/mL6Z (via ) #topicmaps</td>\n",
       "      <td>0.997821</td>\n",
       "      <td>0.950961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Screw you ! I only have 3 weeks...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Screw you ! I only have 3 weeks...</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.983572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aw you would not unfollow me would you? Then I would cry</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Aw you would not unfollow me would you? Then I would cry</td>\n",
       "      <td>0.991137</td>\n",
       "      <td>0.999574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>middle school and elem. High schools will remain open for those who need credits to graduate. Cali is broken</td>\n",
       "      <td>neutral</td>\n",
       "      <td>middle school and elem. High schools will remain open for those who need credits to graduate. Cali is broken</td>\n",
       "      <td>0.994568</td>\n",
       "      <td>0.999130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hey yu lil **** i textd yu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hey yu lil **** i textd yu</td>\n",
       "      <td>0.998491</td>\n",
       "      <td>0.991769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12162</th>\n",
       "      <td>hey guys, if you have something to ask, just ask, okay? we`ll accept your critics and comments. thanks guys</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hey guys, if you have something to ask, just ask, okay? we`ll accept your critics and comments. thanks guys</td>\n",
       "      <td>0.986107</td>\n",
       "      <td>0.988612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12163</th>\n",
       "      <td>not really just leaving flat now, on the lookout for lunch fancy having a wee stroll but dunno where... Oh well!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>not really just leaving flat now, on the lookout for lunch fancy having a wee stroll but dunno where... Oh well!</td>\n",
       "      <td>0.997334</td>\n",
       "      <td>0.974085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12164</th>\n",
       "      <td>I think the lesson of the day is not to have luggage</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I think the lesson of the day is not to have luggage</td>\n",
       "      <td>0.999124</td>\n",
       "      <td>0.999350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12165</th>\n",
       "      <td>haha, yeah. Twitter has many uses. For me it`s just to know what the ppl i care about are doing</td>\n",
       "      <td>neutral</td>\n",
       "      <td>haha, yeah. Twitter has many uses. For me it`s just to know what the ppl i care about are doing</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>0.993949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12166</th>\n",
       "      <td>Succesfully following Tayla!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>Succesfully</td>\n",
       "      <td>0.988908</td>\n",
       "      <td>0.746229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3750 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                    text  \\\n",
       "5       No Topic Maps talks at the Balisage Markup Conference 2009 Program online at http://tr.im/mL6Z (via ) #topicmaps   \n",
       "8       Screw you ! I only have 3 weeks...                                                                                 \n",
       "9       Aw you would not unfollow me would you? Then I would cry                                                           \n",
       "12      middle school and elem. High schools will remain open for those who need credits to graduate. Cali is broken       \n",
       "13      hey yu lil **** i textd yu                                                                                         \n",
       "...                            ...                                                                                         \n",
       "12162   hey guys, if you have something to ask, just ask, okay? we`ll accept your critics and comments. thanks guys        \n",
       "12163   not really just leaving flat now, on the lookout for lunch fancy having a wee stroll but dunno where... Oh well!   \n",
       "12164   I think the lesson of the day is not to have luggage                                                               \n",
       "12165   haha, yeah. Twitter has many uses. For me it`s just to know what the ppl i care about are doing                    \n",
       "12166   Succesfully following Tayla!!                                                                                      \n",
       "\n",
       "      sentiment  \\\n",
       "5      neutral    \n",
       "8      neutral    \n",
       "9      neutral    \n",
       "12     neutral    \n",
       "13     neutral    \n",
       "...        ...    \n",
       "12162  neutral    \n",
       "12163  neutral    \n",
       "12164  neutral    \n",
       "12165  neutral    \n",
       "12166  positive   \n",
       "\n",
       "                                                                                                           selected_text  \\\n",
       "5       No Topic Maps talks at the Balisage Markup Conference 2009 Program online at http://tr.im/mL6Z (via ) #topicmaps   \n",
       "8       Screw you ! I only have 3 weeks...                                                                                 \n",
       "9       Aw you would not unfollow me would you? Then I would cry                                                           \n",
       "12      middle school and elem. High schools will remain open for those who need credits to graduate. Cali is broken       \n",
       "13      hey yu lil **** i textd yu                                                                                         \n",
       "...                            ...                                                                                         \n",
       "12162   hey guys, if you have something to ask, just ask, okay? we`ll accept your critics and comments. thanks guys        \n",
       "12163   not really just leaving flat now, on the lookout for lunch fancy having a wee stroll but dunno where... Oh well!   \n",
       "12164   I think the lesson of the day is not to have luggage                                                               \n",
       "12165   haha, yeah. Twitter has many uses. For me it`s just to know what the ppl i care about are doing                    \n",
       "12166   Succesfully                                                                                                        \n",
       "\n",
       "       start_max   end_max  \n",
       "5      0.997821   0.950961  \n",
       "8      0.999639   0.983572  \n",
       "9      0.991137   0.999574  \n",
       "12     0.994568   0.999130  \n",
       "13     0.998491   0.991769  \n",
       "...         ...        ...  \n",
       "12162  0.986107   0.988612  \n",
       "12163  0.997334   0.974085  \n",
       "12164  0.999124   0.999350  \n",
       "12165  0.997363   0.993949  \n",
       "12166  0.988908   0.746229  \n",
       "\n",
       "[3750 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train_df[meta_train_df['start_max'] >= 0.7][meta_train_df['end_max'] >= 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_train_df = pd.read_csv(os.path.join(config.DATA_DIR, config.TRAIN_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "extend_df = ori_train_df[['text', 'selected_text', 'sentiment']].append(meta_train_df[meta_train_df['start_max'] >= 0.7][meta_train_df['end_max'] >= 0.7][['text', 'selected_text', 'sentiment']], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "extend_df.to_csv(os.path.join(config.DATA_DIR, 'extend_data_pseudo_label.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
